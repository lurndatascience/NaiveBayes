{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee087a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870f256",
   "metadata": {},
   "source": [
    "- Decison Tree is same as like the way we make decisions in day to day life.\n",
    "- It is one of supervised ML algo.\n",
    "- It looks like an if-else ladder code in python.\n",
    "- It is tree based algorithm. \n",
    "- classification and regression algorithm\n",
    "- e.g. Doctor treating cancer patient deciding which medicines to give on certain decisions\n",
    "- Works better for classification.\n",
    "- Non Parametric\n",
    "- We prefer over logistic regression when there is a lot of categorial-mixed data.\n",
    "- No impact of outliers.\n",
    "- We try to achieve homogenity in DT.\n",
    "- To achieve homoginity we use :\n",
    "    - Entropy = - [ ∑ Pi log Pi ] \n",
    "    - Gini  = 1- [ ∑ Pi^2 ]\n",
    "- By using entropy and gini we calculate Information Gain(IG) of Decision Node.\n",
    "- IG = H(s) - wt.avg * (entropy of each feature)\n",
    "- If IG is maximum then leaf node is tending to be pure.\n",
    "- Leaf node carries the classification of decision i.e. output.\n",
    "- Decision node used to make decision.\n",
    "- If IG or Gini has minimum value i.e less impurtiy then Root or Decision node is best node.\n",
    "- Decision Tree tends to overfit a lot.\n",
    "- To overcome overfitting we use regularization :\n",
    "    - Tree Truncation : Hyperparameter Tuning : Min sample split, Min sample leaf, max depth\n",
    "    - Tree Pruning : Cost Complexity Pruning i.e. CCP alpha. \n",
    "        As ccp_alpha values get increases the leaf node will get pruning.\n",
    "\n",
    "\n",
    "- **Advantages and Disadvantages**\n",
    "    - Advantages : \n",
    "        - 1.No effect of outliers.\n",
    "        - 2.Classification and regression.\n",
    "        - 3.Non Parametric\n",
    "        - 4.Easy to implement and understand.\n",
    "        - 5.We can visualize the tree.\n",
    "        - 6.Scaling is not required.\n",
    "    - Disadvantages :\n",
    "        - 1. Tends to overfit a lot.\n",
    "        - 2. Low Bias and High Variance mostly.\n",
    "        - 3. Unstable a lot."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9b695c1",
   "metadata": {},
   "source": [
    "#- ID3 for multiclass.\n",
    "# - CART for binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf49552",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40178ed5",
   "metadata": {},
   "source": [
    "- We have seen in decision tree topic that decision tree tends to overfit a lot that is we use RF.\n",
    "- Every forest is made up of trees and RF is no exception for that cause RF is made up of DTs.\n",
    "- Works very well than many other ML algos.\n",
    "- RF follows Ensemble technique from which it uses Bagging.\n",
    "- Bagging is nothing but the Bootstrap Aggregation. B from Bootstrap Agg from Aggregation.\n",
    "- Easy to Parallelize\n",
    "- OOB error and OOB Score : how our model behaves against testing data.\n",
    "- Typically 2/3rd of data is row sampled.\n",
    "- Typically root(features) or log2(features) sampled in feature sampling.\n",
    "- Hard Voting and Soft Voting. By default SKLearn uses hard voting.\n",
    "- For Ensemble 2 conditions should be satiesfied to accept the Ensembled model:\n",
    "    1. Diversity : Model should be diverse.\n",
    "    2. Acceptiblity : Model should be acceptable enough.\n",
    "- For both regression and classification.\n",
    "- For regression it uses median or mean.SKLearn by default uses Mean.\n",
    "- In DT the model suppreses one of the attribute but, by feature sampling every feature gets equal importance.\n",
    "- In hyperparameter tuning it comprises of n_esimators along with all DT hyperparameters./\n",
    "- **Advantages**:\n",
    "    1. It reduces Overfitting of DTs.\n",
    "    2. Doesn't affect by outliers.\n",
    "    3. Non-Parametric.\n",
    "    4. Feature Scaling is not required.\n",
    "    5. It imporves the testing accuracy.\n",
    "    6. Regression and classification both.\n",
    "    7. Doesn't suppress the attribute like DTs.\n",
    "    8. Easy to parallelize\n",
    "    9. Stable.\n",
    "    10. Works well on high dimensional data.\n",
    "- **Disadvantages**:\n",
    "    1. More Computation req.\n",
    "    2. More Time Req.\n",
    "    3. Black Box Model\n",
    "    4. You can't explain its mathematical intutions in layman's  language.\n",
    "    5. Highly Complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc27ce2",
   "metadata": {},
   "source": [
    "# Adaboost :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737063ae",
   "metadata": {},
   "source": [
    "- When nothing works Boosting does.\n",
    "- Supervised ML algo.\n",
    "- Ensemble sequential approach.\n",
    "- Adaptive Boosting.\n",
    "- One of most amazing idea in ML.\n",
    "- Why we call it booting => we boost the weak learner i.e. decision stump.\n",
    "- What is Weak Learner? => It performs better than random model and it comprises of 2 leaf and 1 decision node.\n",
    "- Every decision stump takes an attribute. Decision stumps called as shallow trees.\n",
    "- Decision stump accuracy is near by 0.6\n",
    "- Due to sequential approach nth model will be dependent of (n-1)th model \n",
    "    which is made by previous stump mistake taking in the account.\n",
    "- All weak learners contribute in order to make a strong learner.\n",
    "- Sample wt. = 1/n\n",
    "- Total Error(TE) =  (Num Misclassified Data pts)/(Total Num Data pts)\n",
    "- Model Pf (MP)= 1/2 ( log (1-TE)/TE)\n",
    "- NSW[Misclassified] = oldwt. * e^(MP)\n",
    "- NSW[Classified] = oldwt. * e^(-MP)\n",
    "- We change distribution of data on each iteration.\n",
    "- Due to bucket system our model chooses more misclassified values rather than correctly classified values.\n",
    "\n",
    "- **Advantages and Disadvantages** :\n",
    "    - Advantages:\n",
    "        1. Time complexity is less.\n",
    "        2. Provides better accuracy than many other ML models.\n",
    "        3. Less likely to be overfitted.\n",
    "        4. Classification and regression.\n",
    "        5. No scaling is required.\n",
    "        6. The accuracy of weak classifiers can be improved by using Adaboost.\n",
    "        7. Non Parametric.\n",
    "        8. Easy to understand and implement.\n",
    "    - Disadvantages :\n",
    "        1. The main disadvantage of Adaboost is that it needs a quality dataset.\n",
    "        2. Slower than XGBoost.\n",
    "        3. Outliers and noisy data should be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "     X1      X2     X3     Y       SW      NSW      SW2        Buckets\n",
    "1    Y       Y     200    Yes      1/8     0.05     0.07     0.00 to 0.07          \n",
    "2    N       Y     180    Yes      1/8     0.05     0.07     0.07 to 0.14\n",
    "3    Y       N     210    Yes      1/8     0.05     0.07     0.14 to 0.21\n",
    "4    Y       Y     167    Yes      1/8     0.33     0.48     0.21 to 0.69\n",
    "5    N       Y     156    No       1/8     0.05     0.07     0.69 to 0.76\n",
    "6    N       Y     125    No       1/8     0.05     0.07     0.76 to 0.83\n",
    "7    Y       N     168    No       1/8     0.05     0.07     0.83 to 0.9\n",
    "8    Y       Y     172    No       1/8     0.05     0.07     0.9  to 1.0\n",
    "\n",
    "\n",
    "\n",
    "0.1  2   N       Y     180    Yes\n",
    "0.2  3   Y       N     210    Yes \n",
    "0.3  4   Y       Y     167    Yes\n",
    "0.4  4   Y       Y     167    Yes\n",
    "0.5  4   Y       Y     167    Yes\n",
    "0.6  4   Y       Y     167    Yes\n",
    "0.7  5   N       Y     156    No\n",
    "0.8  6   N       Y     125    No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76e270",
   "metadata": {},
   "source": [
    "# Naive Bayes :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47dcdc",
   "metadata": {},
   "source": [
    "- Supervised ML algo.\n",
    "- It uses Bayes theorem :\n",
    "    ![Bayes](https://raw.githubusercontent.com/odubno/gauss-naive-bayes/master/img/bayes_1.JPG )\n",
    "- It uses for classification only.\n",
    "- Features should be mutually independent.\n",
    "- All of the features are assumed to be independent when calculating the likelihood; hence \"Naive\"\n",
    "-  Likelihood is calculated using the Gaussian Distribution (Normal Distribution) and all of the features are assumed to be normally distribtuted; hence \"Gauss\"\n",
    "- For problems like email spam filter and text classification Naive Bayes works like a charm.\n",
    "- Laplace Transformation : value of alpha is 1 by default in scikit learn. It also known as pseudocounts.\n",
    "    e.g. E-mail filtering :  p(Lunch|Normalemail) = 0.29  p(Lunch|Spamemail) = 0.\n",
    "- Stopwords : If using for text classification we are removing Stopwords.(NLTK:179) (spaCy:326)\n",
    "- Stemming :(Fin,Finally, Final, Finalize) Stemming has no obligation with meaning and will etract common characters.\n",
    "            Stemming uses the stem of the word. \n",
    "- Lemmatization : Lemmatization uses the context in which the word is being used. Doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word.\n",
    "- Bag of words : A representation of text that describes the occurrence of words within a document. Text to numerical data in matrix.\n",
    "- TF-IDF : We assign weights to individual words.\n",
    "  weight =  log(no of sentence)/(freq. of word in that word)\n",
    "- Bernoulli Naive bayes is good at handling boolean/binary attributes,while Multinomial Naive bayes is good at handling discrete values and Gaussian naive bayes is good at handling continuous values.\n",
    "- There should be balanced data.\n",
    "\n",
    "- **Advantages** :\n",
    "    - It is useful for the classification problems.\n",
    "    - Handles high dimensional data.\n",
    "    - It will perform well on small datasets.\n",
    "    - No hyperparameter tuning is required.\n",
    "    - Less Time Complexity\n",
    "    - Multiclass.\n",
    "    - If assumption of independence holds profoundly then algo. works better.\n",
    "    - Naive Bayes is better suited for categorical input variables than numerical variables.\n",
    "    \n",
    "- **Disadvantages** :\n",
    "    - Assumption of independence.\n",
    "    - Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
