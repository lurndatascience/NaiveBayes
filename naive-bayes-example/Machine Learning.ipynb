{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee087a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870f256",
   "metadata": {},
   "source": [
    "- Decison Tree is same as like the way we make decisions in day to day life.\n",
    "- It is one of supervised ML algo.\n",
    "- It looks like an if-else ladder code in python.\n",
    "- It is tree based algorithm. \n",
    "- classification and regression algorithm\n",
    "- e.g. Doctor treating cancer patient deciding which medicines to give on certain decisions\n",
    "- Works better for classification.\n",
    "- Non Parametric\n",
    "- We prefer over logistic regression when there is a lot of categorial-mixed data.\n",
    "- No impact of outliers.\n",
    "- We try to achieve homogenity in DT.\n",
    "- To achieve homoginity we use :\n",
    "    - Entropy = - [ ∑ Pi log Pi ] \n",
    "    - Gini  = 1- [ ∑ Pi^2 ]\n",
    "- By using entropy and gini we calculate Information Gain(IG) of Decision Node.\n",
    "- IG = H(s) - wt.avg * (entropy of each feature)\n",
    "- If IG is maximum then leaf node is tending to be pure.\n",
    "- Leaf node carries the classification of decision i.e. output.\n",
    "- Decision node used to make decision.\n",
    "- If IG or Gini has minimum value i.e less impurtiy then Root or Decision node is best node.\n",
    "- Decision Tree tends to overfit a lot.\n",
    "- To overcome overfitting we use regularization :\n",
    "    - Tree Truncation : Hyperparameter Tuning : Min sample split, Min sample leaf, max depth\n",
    "    - Tree Pruning : Cost Complexity Pruning i.e. CCP alpha. \n",
    "        As ccp_alpha values get increases the leaf node will get pruning.\n",
    "\n",
    "\n",
    "- **Advantages and Disadvantages**\n",
    "    - Advantages : \n",
    "        - 1.No effect of outliers.\n",
    "        - 2.Classification and regression.\n",
    "        - 3.Non Parametric\n",
    "        - 4.Easy to implement and understand.\n",
    "        - 5.We can visualize the tree.\n",
    "        - 6.Scaling is not required.\n",
    "    - Disadvantages :\n",
    "        - 1. Tends to overfit a lot.\n",
    "        - 2. Low Bias and High Variance mostly.\n",
    "        - 3. Unstable a lot."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9b695c1",
   "metadata": {},
   "source": [
    "#- ID3 for multiclass.\n",
    "# - CART for binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf49552",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40178ed5",
   "metadata": {},
   "source": [
    "- We have seen in decision tree topic that decision tree tends to overfit a lot that is we use RF.\n",
    "- Every forest is made up of trees and RF is no exception for that cause RF is made up of DTs.\n",
    "- Works very well than many other ML algos.\n",
    "- RF follows Ensemble technique from which it uses Bagging.\n",
    "- Bagging is nothing but the Bootstrap Aggregation. B from Bootstrap Agg from Aggregation.\n",
    "- Easy to Parallelize\n",
    "- OOB error and OOB Score : how our model behaves against testing data.\n",
    "- Typically 2/3rd of data is row sampled.\n",
    "- Typically root(features) or log2(features) sampled in feature sampling.\n",
    "- Hard Voting and Soft Voting. By default SKLearn uses hard voting.\n",
    "- For Ensemble 2 conditions should be satiesfied to accept the Ensembled model:\n",
    "    1. Diversity : Model should be diverse.\n",
    "    2. Acceptiblity : Model should be acceptable enough.\n",
    "- For both regression and classification.\n",
    "- For regression it uses median or mean.SKLearn by default uses Mean.\n",
    "- In DT the model suppreses one of the attribute but, by feature sampling every feature gets equal importance.\n",
    "- In hyperparameter tuning it comprises of n_esimators along with all DT hyperparameters./\n",
    "- **Advantages**:\n",
    "    1. It reduces Overfitting of DTs.\n",
    "    2. Doesn't affect by outliers.\n",
    "    3. Non-Parametric.\n",
    "    4. Feature Scaling is not required.\n",
    "    5. It imporves the testing accuracy.\n",
    "    6. Regression and classification both.\n",
    "    7. Doesn't suppress the attribute like DTs.\n",
    "    8. Easy to parallelize\n",
    "    9. Stable.\n",
    "    10. Works well on high dimensional data.\n",
    "- **Disadvantages**:\n",
    "    1. More Computation req.\n",
    "    2. More Time Req.\n",
    "    3. Black Box Model\n",
    "    4. You can't explain its mathematical intutions in layman's  language.\n",
    "    5. Highly Complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc27ce2",
   "metadata": {},
   "source": [
    "# Adaboost :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737063ae",
   "metadata": {},
   "source": [
    "- When nothing works Boosting does.\n",
    "- Supervised ML algo.\n",
    "- Ensemble sequential approach.\n",
    "- Adaptive Boosting.\n",
    "- One of most amazing idea in ML.\n",
    "- Why we call it booting => we boost the weak learner i.e. decision stump.\n",
    "- What is Weak Learner? => It performs better than random model and it comprises of 2 leaf and 1 decision node.\n",
    "- Every decision stump takes an attribute. Decision stumps called as shallow trees.\n",
    "- Decision stump accuracy is near by 0.6\n",
    "- Due to sequential approach nth model will be dependent of (n-1)th model \n",
    "    which is made by previous stump mistake taking in the account.\n",
    "- All weak learners contribute in order to make a strong learner.\n",
    "- Sample wt. = 1/n\n",
    "- Total Error(TE) =  (Num Misclassified Data pts)/(Total Num Data pts)\n",
    "- Model Pf (MP)= 1/2 ( log (1-TE)/TE)\n",
    "- NSW[Misclassified] = oldwt. * e^(MP)\n",
    "- NSW[Classified] = oldwt. * e^(-MP)\n",
    "- We change distribution of data on each iteration.\n",
    "- Due to bucket system our model chooses more misclassified values rather than correctly classified values.\n",
    "\n",
    "- **Advantages and Disadvantages** :\n",
    "    - Advantages:\n",
    "        1. Time complexity is less.\n",
    "        2. Provides better accuracy than many other ML models.\n",
    "        3. Less likely to be overfitted.\n",
    "        4. Classification and regression.\n",
    "        5. No scaling is required.\n",
    "        6. The accuracy of weak classifiers can be improved by using Adaboost.\n",
    "        7. Non Parametric.\n",
    "        8. Easy to understand and implement.\n",
    "    - Disadvantages :\n",
    "        1. The main disadvantage of Adaboost is that it needs a quality dataset.\n",
    "        2. Slower than XGBoost.\n",
    "        3. Outliers and noisy data should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76e270",
   "metadata": {},
   "source": [
    "# Naive Bayes :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47dcdc",
   "metadata": {},
   "source": [
    "- Supervised ML algo.\n",
    "- It uses Bayes theorem :\n",
    "<img src=\"https://raw.githubusercontent.com/odubno/gauss-naive-bayes/master/img/bayes_1.JPG\" width=\"400\"/>\n",
    "\n",
    "- It uses for classification only.\n",
    "- Features should be mutually independent.\n",
    "- All of the features are assumed to be independent when calculating the likelihood; hence \"Naive\"\n",
    "-  Likelihood is calculated using the Gaussian Distribution (Normal Distribution) and all of the features are assumed to be normally distribtuted; hence \"Gauss\"\n",
    "- For problems like email spam filter and text classification Naive Bayes works like a charm.\n",
    "- Laplace Transformation : value of alpha is 1 by default in scikit learn. It also known as pseudocounts.\n",
    "    e.g. E-mail filtering :  p(Lunch|Normalemail) = 0.29  p(Lunch|Spamemail) = 0.\n",
    "- Stopwords : If using for text classification we are removing Stopwords.(NLTK:179) (spaCy:326)\n",
    "- Stemming :(Fin,Finally, Final, Finalize) Stemming has no obligation with meaning and will extract common characters.\n",
    "            Stemming uses the stem of the word. \n",
    "- Lemmatization : Lemmatization uses the context in which the word is being used. Doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word.\n",
    "- Bag of words : A representation of text that describes the occurrence of words within a document. Text to numerical data in matrix.\n",
    "- TF-IDF : We assign weights to individual words.\n",
    "  weight =  log(no of sentence)/(freq. of word in that word)\n",
    "- Bernoulli Naive bayes is good at handling boolean/binary attributes,while Multinomial Naive bayes is good at handling discrete values and Gaussian naive bayes is good at handling continuous values.\n",
    "- There should be balanced data.\n",
    "\n",
    "- **Advantages** :\n",
    "    - It is useful for the classification problems.\n",
    "    - Handles high dimensional data.\n",
    "    - It will perform well on small datasets.\n",
    "    - No hyperparameter tuning is required.\n",
    "    - Less Time Complexity\n",
    "    - Multiclass.\n",
    "    - If assumption of independence holds profoundly then algo. works better.\n",
    "    - Naive Bayes is better suited for categorical input variables than numerical variables.\n",
    "    \n",
    "- **Disadvantages** :\n",
    "    - Assumption of independence.\n",
    "    - Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77a730",
   "metadata": {},
   "source": [
    "# Support Vector Machines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0f86f",
   "metadata": {},
   "source": [
    "- SVM can be used to solve both classification and regression.\n",
    "- It is one of supervised ML algo.\n",
    "- SVM creates marginal lines, which is equally saperable from the hyperplane.\n",
    "- It chooses best line to divide to points into two groups this line often termed as hyperplane.\n",
    "- SVM has support vectors. And if you ask me what does the support vectors do? Support vectors are the points from distict data groups through which the marginal plane passes. And marginal planes are the planes which helps us to maximise the margin between two data groups.\n",
    "\n",
    "- SVM is supervised machine learning algorithm.\n",
    "- SVM is margin based algoritham .\n",
    "- SVM is used for Classification as well as Regression problems.\n",
    "- SVM is a powerful supervised algorithm that works best on smaller datasets but on complex ones.\n",
    "- The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points.\n",
    "- SVM produces a hyperplane separating two classes of data using the largest possible margin to the data\n",
    "- SVMs are sensitive to feature scaling as it takes input data to find the margins around hyperplanes and gets biased for the     variance in high values.\n",
    "\n",
    "\n",
    "\n",
    "**Margin:**\n",
    "- It is the distance between the hyperplane and the observations closest to the hyperplane (support vectors). In SVM large margin is considered a good margin\n",
    "        \n",
    "**There are two types margin:-**\n",
    "\n",
    "**Hard-Margin:-**\n",
    " - Hard-Margin SVMs have linearly separable training data. No data points are allowed in the margin areas. This type of linear     classification is known as Hard margin classification.\n",
    "\n",
    "**Soft-Margin:-**\n",
    "- Soft-Margin SVMs have training data that are not linearly separable. Margin violation means choosing a hyperplane, which can allow some data points to stay either in between the margin area or on the incorrect side of the hyperplane.\n",
    "\n",
    "**Hyperplane**:\n",
    "- 1.A hyperplane is a decision boundary that differentiates the two classes in SVM.\n",
    "- 2.A data point falling on either side of the hyperplane can be attributed to different classes.\n",
    "- 3.The dimension of the hyperplane depends on the number of input features in the dataset.\n",
    "\n",
    "**Its aimed at finding an optimal hyperplane that is linearly separable,and for the dataset which is not directly linearly \n",
    "separable,it extends its formulation by transforming the original data to map into a new space,which is also called kernel trick**\n",
    "\n",
    "##### Kernel Trick:\n",
    "- Most datasets are not linearly saperable in real life.\n",
    "- The simplicity of SVMs can be problamatic in some cases. Suppose there is a non linear saperable data. But as we know we cant saperate the data with good old SVMs. thats why we follow 3 steps :\n",
    "    1. Augmented non-linear data is saperated in high dimensional space.\n",
    "    2. Find exact optimum 3D plane dividing 2 or more classes.\n",
    "    3. Project the data back to original space.\n",
    "- As we can see all these steps are tedious and complex. But Kernal trick comes handy in this situation. Kernal Trick divides the plane.\n",
    "\n",
    "\n",
    "### Advantages :\n",
    "- 1.SVM works relatively well when there is a clear margin of separation between classes.\n",
    "- 2.SVM is more effective in high dimensional spaces.\n",
    "- 3.SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "- 4.SVM is relatively memory efficient\n",
    "\n",
    "### Disadvantages :\n",
    "- 1.SVM algorithm is not suitable for large data sets.\n",
    "- 2.SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "- 3.In cases where the number of features for each data point exceeds the number of training data samples, the SVM will \n",
    "  underperform.\n",
    "- 4.As the support vector classifier works by putting data points, above and below the classifying hyperplane there is no \n",
    "  probabilistic explanation for the classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
